{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70af158f",
   "metadata": {},
   "source": [
    "# NOTEBOOK 6: MATRIX FACTORIZATION (SVD)\n",
    "## Latent Factor Models for Recommendations\n",
    "\n",
    "This notebook implements matrix factorization using Singular Value Decomposition (SVD) to uncover latent features in the user-item interaction matrix.\n",
    "It includes training SVD models with different dimensions, generating recommendations, evaluating performance, visualizing the latent space, and comparing results with content-based and collaborative filtering methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa1b0ba",
   "metadata": {},
   "source": [
    "## 1. SETUP & CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "955378df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SETUP: IMPORTING LIBRARIES\n",
      "================================================================================\n",
      " All libraries imported successfully\n",
      " Plot settings configured\n",
      "\n",
      "================================================================================\n",
      "DIRECTORY SETUP\n",
      "================================================================================\n",
      " Current directory: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\notebooks\n",
      " Project root: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\n",
      "\n",
      " Directory structure:\n",
      "   • Data: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\data\n",
      "   • Processed: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\data\\processed\n",
      "   • Models: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\models\n",
      "   • Results: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\reports\\results\n",
      " All directories ready\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA FROM PREVIOUS NOTEBOOKS\n",
      "================================================================================\n",
      "\n",
      " Loading user-item interaction matrix...\n",
      "   Looking for: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\models\\user_item_matrix.npz\n",
      "    Matrix loaded from file\n",
      "\n",
      " Matrix ready:\n",
      "   Shape: 137,883 users × 34,461 movies\n",
      "   Non-zero entries: 20,000,076\n",
      "   Sparsity: 99.58%\n",
      "   Memory: 152.6 MB\n",
      "\n",
      "  Loading user/movie ID mappings...\n",
      "    Mappings loaded from file\n",
      "   Users: 137,883\n",
      "   Movies: 34,461\n",
      "\n",
      " Loading evaluation setup...\n",
      " Evaluation setup loaded:\n",
      "   Evaluation users: 100\n",
      "   Ground truth available: 100 users\n",
      "\n",
      " Previous CF Results (for comparison):\n",
      "   User-Based CF:\n",
      "      • Precision@10: 1.73%\n",
      "      • Hit Rate@10:  16.33%\n",
      "   Item-Based CF:\n",
      "      • Precision@10: 14.08%\n",
      "      • Hit Rate@10:  48.98%\n",
      "\n",
      " Loading movies data...\n",
      " Movies loaded: 62,423 movies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# IMPORT LIBRARIES\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SETUP: IMPORTING LIBRARIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "import numpy as np              \n",
    "import pandas as pd            \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns          \n",
    "import pickle                \n",
    "import os                     \n",
    "import time                    \n",
    "from datetime import datetime  \n",
    "\n",
    "# Sparse matrix operations\n",
    "from scipy.sparse import csr_matrix, load_npz  \n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import TruncatedSVD  \n",
    "from sklearn.preprocessing import normalize     \n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" All libraries imported successfully\")\n",
    "\n",
    " \n",
    "# VISUALIZATION SETTINGS\n",
    "\n",
    "\n",
    "# Set consistent, professional plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Default figure settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "print(\" Plot settings configured\")\n",
    "\n",
    "\n",
    "# DIRECTORY STRUCTURE \n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIRECTORY SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())  \n",
    "\n",
    "print(f\" Current directory: {os.getcwd()}\")\n",
    "print(f\" Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Define project directories relative to PROJECT_ROOT\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "PROCESSED_DIR = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'reports', 'results')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [MODELS_DIR, RESULTS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "print(f\"\\n Directory structure:\")\n",
    "print(f\"   • Data: {DATA_DIR}\")\n",
    "print(f\"   • Processed: {PROCESSED_DIR}\")\n",
    "print(f\"   • Models: {MODELS_DIR}\")\n",
    "print(f\"   • Results: {RESULTS_DIR}\")\n",
    "\n",
    "# Verify directories exist\n",
    "all_exist = all(os.path.exists(d) for d in [DATA_DIR, MODELS_DIR])\n",
    "if all_exist:\n",
    "    print(f\" All directories ready\")\n",
    "else:\n",
    "    print(f\"  Some directories missing - will try to continue\")\n",
    "\n",
    "# LOAD PREVIOUS RESULTS\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING DATA FROM PREVIOUS NOTEBOOKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load User-Item Matrix\n",
    "\n",
    "print(\"\\n Loading user-item interaction matrix...\")\n",
    "\n",
    "matrix_path = os.path.join(MODELS_DIR, 'user_item_matrix.npz')\n",
    "\n",
    "print(f\"   Looking for: {matrix_path}\")\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(matrix_path):\n",
    "    print(f\"    Matrix not found!\")\n",
    "    print(f\"\\n  SOLUTION: We need to generate the matrix from Notebook 5 data\")\n",
    "    print(f\"   I'll help you create it now...\\n\")\n",
    "    \n",
    "    # Try to load from training data\n",
    "    train_path = os.path.join(DATA_DIR, 'ratings_train.csv')\n",
    "    \n",
    "    if os.path.exists(train_path):\n",
    "        print(\"   ✅ Found training data - will regenerate matrix\")\n",
    "        print(\"   This will take a few minutes...\")\n",
    "        \n",
    "        # Load training data\n",
    "        train = pd.read_csv(train_path)\n",
    "        print(f\"   Loaded: {len(train):,} training ratings\")\n",
    "        \n",
    "        # Create mappings\n",
    "        unique_users = train['userId'].unique()\n",
    "        unique_movies = train['movieId'].unique()\n",
    "        \n",
    "        user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "        idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}\n",
    "        movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(unique_movies)}\n",
    "        idx_to_movie = {idx: movie_id for movie_id, idx in movie_to_idx.items()}\n",
    "        \n",
    "        print(f\"   Created mappings: {len(user_to_idx):,} users, {len(movie_to_idx):,} movies\")\n",
    "        \n",
    "        # Build matrix\n",
    "        n_users = len(user_to_idx)\n",
    "        n_movies = len(movie_to_idx)\n",
    "        \n",
    "        user_indices = train['userId'].map(user_to_idx).values\n",
    "        movie_indices = train['movieId'].map(movie_to_idx).values\n",
    "        ratings = train['rating'].values\n",
    "        \n",
    "        user_item_matrix = csr_matrix(\n",
    "            (ratings, (user_indices, movie_indices)),\n",
    "            shape=(n_users, n_movies)\n",
    "        )\n",
    "        \n",
    "        print(f\"   Matrix created: {user_item_matrix.shape}\")\n",
    "        \n",
    "        # Save for next time\n",
    "        from scipy.sparse import save_npz\n",
    "        save_npz(matrix_path, user_item_matrix)\n",
    "        print(f\"    Saved matrix for future use\")\n",
    "        \n",
    "        # Save mappings\n",
    "        mappings = {\n",
    "            'user_to_idx': user_to_idx,\n",
    "            'idx_to_user': idx_to_user,\n",
    "            'movie_to_idx': movie_to_idx,\n",
    "            'idx_to_movie': idx_to_movie\n",
    "        }\n",
    "        mappings_path = os.path.join(MODELS_DIR, 'matrix_mappings.pkl')\n",
    "        with open(mappings_path, 'wb') as f:\n",
    "            pickle.dump(mappings, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"    Saved mappings for future use\")\n",
    "        \n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cannot find training data at {train_path}. \"\n",
    "            \"Please ensure Notebook 5 has been run and data files exist.\"\n",
    "        )\n",
    "else:\n",
    "    # Load existing matrix\n",
    "    user_item_matrix = load_npz(matrix_path)\n",
    "    print(f\"    Matrix loaded from file\")\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = (1 - user_item_matrix.nnz / \n",
    "            (user_item_matrix.shape[0] * user_item_matrix.shape[1])) * 100\n",
    "\n",
    "print(f\"\\n Matrix ready:\")\n",
    "print(f\"   Shape: {user_item_matrix.shape[0]:,} users × \"\n",
    "      f\"{user_item_matrix.shape[1]:,} movies\")\n",
    "print(f\"   Non-zero entries: {user_item_matrix.nnz:,}\")\n",
    "print(f\"   Sparsity: {sparsity:.2f}%\")\n",
    "print(f\"   Memory: {user_item_matrix.data.nbytes / (1024**2):.1f} MB\")\n",
    "\n",
    "#\n",
    "# Load ID Mappings\n",
    "\n",
    "print(\"\\n  Loading user/movie ID mappings...\")\n",
    "\n",
    "mappings_path = os.path.join(MODELS_DIR, 'matrix_mappings.pkl')\n",
    "\n",
    "if not os.path.exists(mappings_path):\n",
    "    print(f\"     Mappings not found - using ones created above\")\n",
    "    # Already created above when regenerating matrix\n",
    "else:\n",
    "    with open(mappings_path, 'rb') as f:\n",
    "        mappings = pickle.load(f)\n",
    "        user_to_idx = mappings['user_to_idx']\n",
    "        idx_to_user = mappings['idx_to_user']\n",
    "        movie_to_idx = mappings['movie_to_idx']\n",
    "        idx_to_movie = mappings['idx_to_movie']\n",
    "    print(f\"    Mappings loaded from file\")\n",
    "\n",
    "print(f\"   Users: {len(user_to_idx):,}\")\n",
    "print(f\"   Movies: {len(movie_to_idx):,}\")\n",
    "\n",
    "# Load Evaluation Setup \n",
    "\n",
    "print(\"\\n Loading evaluation setup...\")\n",
    "\n",
    "cf_results_path = os.path.join(MODELS_DIR, 'evaluation_results_cf.pkl')\n",
    "\n",
    "if os.path.exists(cf_results_path):\n",
    "    with open(cf_results_path, 'rb') as f:\n",
    "        cf_results = pickle.load(f)\n",
    "        eval_user_ids = cf_results['eval_user_ids']\n",
    "        ground_truth = cf_results['ground_truth']\n",
    "        ub_metrics = cf_results['user_based_metrics']\n",
    "        ib_metrics = cf_results['item_based_metrics']\n",
    "    \n",
    "    print(f\" Evaluation setup loaded:\")\n",
    "    print(f\"   Evaluation users: {len(eval_user_ids)}\")\n",
    "    print(f\"   Ground truth available: {len(ground_truth)} users\")\n",
    "    \n",
    "    print(f\"\\n Previous CF Results (for comparison):\")\n",
    "    print(f\"   User-Based CF:\")\n",
    "    print(f\"      • Precision@10: {ub_metrics['Precision@10']:.2f}%\")\n",
    "    print(f\"      • Hit Rate@10:  {ub_metrics['Hit Rate@10']:.2f}%\")\n",
    "    print(f\"   Item-Based CF:\")\n",
    "    print(f\"      • Precision@10: {ib_metrics['Precision@10']:.2f}%\")\n",
    "    print(f\"      • Hit Rate@10:  {ib_metrics['Hit Rate@10']:.2f}%\")\n",
    "else:\n",
    "    print(f\"     CF results not found\")\n",
    "    print(f\"   We'll create evaluation setup from test data...\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_path = os.path.join(DATA_DIR, 'ratings_test.csv')\n",
    "    if os.path.exists(test_path):\n",
    "        test = pd.read_csv(test_path)\n",
    "        \n",
    "        # Sample 100 users who exist in training\n",
    "        test_users = test[test['userId'].isin(user_to_idx.keys())]['userId'].unique()\n",
    "        np.random.seed(42)\n",
    "        eval_user_ids = np.random.choice(test_users, size=min(100, len(test_users)), replace=False)\n",
    "        \n",
    "        # Create ground truth\n",
    "        ground_truth = {}\n",
    "        for user_id in eval_user_ids:\n",
    "            user_test = test[test['userId'] == user_id]\n",
    "            relevant = user_test[user_test['rating'] >= 4.0]['movieId'].tolist()\n",
    "            ground_truth[user_id] = relevant\n",
    "        \n",
    "        print(f\"    Created evaluation setup: {len(eval_user_ids)} users\")\n",
    "        \n",
    "        # No previous CF metrics\n",
    "        ub_metrics = None\n",
    "        ib_metrics = None\n",
    "    else:\n",
    "        print(f\"    Cannot create evaluation - test data not found\")\n",
    "        eval_user_ids = []\n",
    "        ground_truth = {}\n",
    "        ub_metrics = None\n",
    "        ib_metrics = None\n",
    "\n",
    "\n",
    "# Load Movies Data\n",
    "\n",
    "print(\"\\n Loading movies data...\")\n",
    "\n",
    "movies_path = os.path.join(PROCESSED_DIR, 'movies_features.csv')\n",
    "if not os.path.exists(movies_path):\n",
    "    movies_path = os.path.join(DATA_DIR, 'movies_filtered.csv')\n",
    "if not os.path.exists(movies_path):\n",
    "    movies_path = os.path.join(DATA_DIR, 'movies.csv')\n",
    "\n",
    "if os.path.exists(movies_path):\n",
    "    movies = pd.read_csv(movies_path)\n",
    "    print(f\" Movies loaded: {len(movies):,} movies\")\n",
    "else:\n",
    "    print(f\"  Movies file not found - will work without titles\")\n",
    "    movies = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb3633",
   "metadata": {},
   "source": [
    "## CELL 2: DATA NORMALIZATION FOR SVD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "356429b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      " NORMALIZE USER-ITEM MATRIX\n",
      "================================================================================\n",
      "--------------------------------------------------------------------------------\n",
      "Step 1: Computing user rating averages (only from rated movies)\n",
      "--------------------------------------------------------------------------------\n",
      "   Computed means for 20,000 / 137,883 users...\n",
      "   Computed means for 40,000 / 137,883 users...\n",
      "   Computed means for 60,000 / 137,883 users...\n",
      "   Computed means for 80,000 / 137,883 users...\n",
      "   Computed means for 100,000 / 137,883 users...\n",
      "   Computed means for 120,000 / 137,883 users...\n",
      "\n",
      " Computed means for 137,883 users\n",
      "   Mean of means: 3.65\n",
      "   Std of means:  0.46\n",
      "   Min mean:      0.50 (harsh critic)\n",
      "   Max mean:      5.00 (generous rater)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Step 2: Centering ratings (subtract user means)\n",
      "--------------------------------------------------------------------------------\n",
      "   Centering in progress...\n",
      "   Processed 20,000 / 137,883 users...\n",
      "   Processed 40,000 / 137,883 users...\n",
      "   Processed 60,000 / 137,883 users...\n",
      "   Processed 80,000 / 137,883 users...\n",
      "   Processed 100,000 / 137,883 users...\n",
      "   Processed 120,000 / 137,883 users...\n",
      "\n",
      " Matrix centered successfully\n",
      "   New mean rating: -0.000000★ (should be ≈0)\n",
      "   New std rating:  0.95★\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Step 3: Verification\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sample user 0:\n",
      "   Number of ratings: 62\n",
      "   Original ratings: [3. 5. 3. 5. 5.] ...\n",
      "   User mean: 3.44\n",
      "   Centered ratings: [-0.43548387  1.56451613 -0.43548387  1.56451613  1.56451613] ...\n",
      "   Centered mean: -0.000000 (should be ≈0)\n",
      "\n",
      "Random user 57455:\n",
      "   Number of ratings: 215\n",
      "   Original mean: 3.59\n",
      "   User's saved mean: 3.59\n",
      "   Centered mean: 0.000000 (should be ≈0)\n",
      "\n",
      "============================================================\n",
      " CENTERING SUCCESSFUL!\n",
      "   Overall centered mean: -0.000000 ≈ 0\n",
      "============================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Step 4: Saving normalized data\n",
      "--------------------------------------------------------------------------------\n",
      " Centered matrix saved: user_item_matrix_centered.npz\n",
      " User means saved: user_means.npy\n",
      "\n",
      "Summary:\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n",
      "Original Matrix:\n",
      "  • Shape: 137,883  34,461\n",
      "  • Mean rating: 3.53\n",
      "  • Std rating: 1.06\n",
      "  • Non-zero entries: 20,000,076\n",
      "\n",
      "Centered Matrix:\n",
      "  • Shape: 137,883  34,461\n",
      "  • Mean rating: -0.000000  (≈0)\n",
      "  • Std rating: 0.95\n",
      "  • Non-zero entries: 20,000,076\n",
      "\n",
      "User Rating Statistics:\n",
      "  • Average user rating: 3.65\n",
      "  • Most generous user: 5.00\n",
      "  • Most critical user: 0.50\n",
      "  • Std across users: 0.46\n",
      "\n",
      "Files Saved:\n",
      "   user_item_matrix_centered.npz (for SVD training)\n",
      "   user_means.npy (for prediction denormalization)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" NORMALIZE USER-ITEM MATRIX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# Compute User Rating Means \n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Step 1: Computing user rating averages (only from rated movies)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate mean rating for each user\n",
    "# CRITICAL: Only average over RATED movies, not all movies!\n",
    "user_means = np.zeros(user_item_matrix.shape[0])\n",
    "\n",
    "for user_idx in range(user_item_matrix.shape[0]):\n",
    "    # Get user's ratings (only non-zero)\n",
    "    user_ratings = user_item_matrix.getrow(user_idx).data\n",
    "    \n",
    "    # Calculate mean only from rated movies\n",
    "    if len(user_ratings) > 0:\n",
    "        user_means[user_idx] = user_ratings.mean()\n",
    "    else:\n",
    "        user_means[user_idx] = 0  # Users with no ratings (shouldn't happen)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (user_idx + 1) % 20000 == 0:\n",
    "        print(f\"   Computed means for {user_idx + 1:,} / {user_item_matrix.shape[0]:,} users...\")\n",
    "\n",
    "print(f\"\\n Computed means for {len(user_means):,} users\")\n",
    "print(f\"   Mean of means: {user_means.mean():.2f}\")\n",
    "print(f\"   Std of means:  {user_means.std():.2f}\")\n",
    "print(f\"   Min mean:      {user_means.min():.2f} (harsh critic)\")\n",
    "print(f\"   Max mean:      {user_means.max():.2f} (generous rater)\")\n",
    "\n",
    "# Center the Matrix\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Step 2: Centering ratings (subtract user means)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create centered matrix by subtracting user means\n",
    "n_users, n_movies = user_item_matrix.shape\n",
    "\n",
    "data_centered = []\n",
    "indices_centered = []\n",
    "indptr_centered = [0]\n",
    "\n",
    "print(\"   Centering in progress...\")\n",
    "for user_idx in range(n_users):\n",
    "    # Get user's ratings (sparse row)\n",
    "    user_row = user_item_matrix.getrow(user_idx)\n",
    "    \n",
    "    # Get non-zero indices and values\n",
    "    _, movie_indices = user_row.nonzero()\n",
    "    ratings = user_row.data\n",
    "    \n",
    "    # Center the ratings: subtract this user's mean\n",
    "    centered_ratings = ratings - user_means[user_idx]\n",
    "    \n",
    "    # Store in new sparse format\n",
    "    data_centered.extend(centered_ratings)\n",
    "    indices_centered.extend(movie_indices)\n",
    "    indptr_centered.append(len(data_centered))\n",
    "    \n",
    "    # Progress indicator\n",
    "    if (user_idx + 1) % 20000 == 0:\n",
    "        print(f\"   Processed {user_idx + 1:,} / {n_users:,} users...\")\n",
    "\n",
    "# Create centered sparse matrix\n",
    "user_item_matrix_centered = csr_matrix(\n",
    "    (data_centered, indices_centered, indptr_centered),\n",
    "    shape=(n_users, n_movies)\n",
    ")\n",
    "\n",
    "print(f\"\\n Matrix centered successfully\")\n",
    "print(f\"   New mean rating: {np.array(data_centered).mean():.6f}★ (should be ≈0)\")\n",
    "print(f\"   New std rating:  {np.array(data_centered).std():.2f}★\")\n",
    "\n",
    "\n",
    "# Verify Centering\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Step 3: Verification\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check a sample user who has many ratings\n",
    "sample_user_idx = 0\n",
    "original_ratings = user_item_matrix.getrow(sample_user_idx).data\n",
    "centered_ratings = user_item_matrix_centered.getrow(sample_user_idx).data\n",
    "\n",
    "print(f\"\\nSample user {sample_user_idx}:\")\n",
    "print(f\"   Number of ratings: {len(original_ratings)}\")\n",
    "print(f\"   Original ratings: {original_ratings[:5]} ...\")\n",
    "print(f\"   User mean: {user_means[sample_user_idx]:.2f}\")\n",
    "print(f\"   Centered ratings: {centered_ratings[:5]} ...\")\n",
    "print(f\"   Centered mean: {centered_ratings.mean():.6f} (should be ≈0)\")\n",
    "\n",
    "# Verify on another random user\n",
    "random_user = np.random.randint(0, n_users)\n",
    "orig = user_item_matrix.getrow(random_user).data\n",
    "cent = user_item_matrix_centered.getrow(random_user).data\n",
    "\n",
    "print(f\"\\nRandom user {random_user}:\")\n",
    "print(f\"   Number of ratings: {len(orig)}\")\n",
    "print(f\"   Original mean: {orig.mean():.2f}\")\n",
    "print(f\"   User's saved mean: {user_means[random_user]:.2f}\")\n",
    "print(f\"   Centered mean: {cent.mean():.6f} (should be ≈0)\")\n",
    "\n",
    "# Overall verification\n",
    "overall_mean = np.array(data_centered).mean()\n",
    "print(f\"\\n{'='*60}\")\n",
    "if abs(overall_mean) < 0.01:\n",
    "    print(f\" CENTERING SUCCESSFUL!\")\n",
    "    print(f\"   Overall centered mean: {overall_mean:.6f} ≈ 0\")\n",
    "else:\n",
    "    print(f\"  WARNING: Centering may have issues\")\n",
    "    print(f\"   Overall centered mean: {overall_mean:.6f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Save for Later Use\n",
    "\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"Step 4: Saving normalized data\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Save centered matrix\n",
    "from scipy.sparse import save_npz\n",
    "\n",
    "centered_matrix_path = os.path.join(MODELS_DIR, 'user_item_matrix_centered.npz')\n",
    "save_npz(centered_matrix_path, user_item_matrix_centered)\n",
    "print(f\" Centered matrix saved: {os.path.basename(centered_matrix_path)}\")\n",
    "\n",
    "# Save user means (needed for denormalization later)\n",
    "user_means_path = os.path.join(MODELS_DIR, 'user_means.npy')\n",
    "np.save(user_means_path, user_means)\n",
    "print(f\" User means saved: {os.path.basename(user_means_path)}\")\n",
    "\n",
    "# Summary\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "Summary:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "Original Matrix:\n",
    "  • Shape: {user_item_matrix.shape[0]:,}  {user_item_matrix.shape[1]:,}\n",
    "  • Mean rating: {user_item_matrix.data.mean():.2f}\n",
    "  • Std rating: {user_item_matrix.data.std():.2f}\n",
    "  • Non-zero entries: {user_item_matrix.nnz:,}\n",
    "\n",
    "Centered Matrix:\n",
    "  • Shape: {user_item_matrix_centered.shape[0]:,}  {user_item_matrix_centered.shape[1]:,}\n",
    "  • Mean rating: {np.array(data_centered).mean():.6f} {\" (≈0)\" if abs(np.array(data_centered).mean()) < 0.01 else \"\"}\n",
    "  • Std rating: {np.array(data_centered).std():.2f}\n",
    "  • Non-zero entries: {user_item_matrix_centered.nnz:,}\n",
    "\n",
    "User Rating Statistics:\n",
    "  • Average user rating: {user_means.mean():.2f}\n",
    "  • Most generous user: {user_means.max():.2f}\n",
    "  • Most critical user: {user_means.min():.2f}\n",
    "  • Std across users: {user_means.std():.2f}\n",
    "\n",
    "Files Saved:\n",
    "   user_item_matrix_centered.npz (for SVD training)\n",
    "   user_means.npy (for prediction denormalization)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170b2e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b673f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

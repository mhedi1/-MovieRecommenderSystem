{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70af158f",
   "metadata": {},
   "source": [
    "# NOTEBOOK 6: MATRIX FACTORIZATION (SVD)\n",
    "## Latent Factor Models for Recommendations\n",
    "\n",
    "This notebook implements matrix factorization using Singular Value Decomposition (SVD) to uncover latent features in the user-item interaction matrix.\n",
    "It includes training SVD models with different dimensions, generating recommendations, evaluating performance, visualizing the latent space, and comparing results with content-based and collaborative filtering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "955378df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SETUP: IMPORTING LIBRARIES\n",
      "================================================================================\n",
      " All libraries imported successfully\n",
      " Plot settings configured\n",
      "\n",
      "================================================================================\n",
      "DIRECTORY SETUP\n",
      "================================================================================\n",
      " Current directory: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\notebooks\n",
      " Project root: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\n",
      "\n",
      " Directory structure:\n",
      "   • Data: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\data\n",
      "   • Processed: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\data\\processed\n",
      "   • Models: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\models\n",
      "   • Results: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\reports\\results\n",
      " All directories ready\n",
      "\n",
      "================================================================================\n",
      "LOADING DATA FROM PREVIOUS NOTEBOOKS\n",
      "================================================================================\n",
      "\n",
      " Loading user-item interaction matrix...\n",
      "   Looking for: c:\\Users\\mhfou\\Documents\\MovieRecommenderSystem\\models\\user_item_matrix.npz\n",
      "    Matrix loaded from file\n",
      "\n",
      " Matrix ready:\n",
      "   Shape: 137,883 users × 34,461 movies\n",
      "   Non-zero entries: 20,000,076\n",
      "   Sparsity: 99.58%\n",
      "   Memory: 152.6 MB\n",
      "\n",
      "  Loading user/movie ID mappings...\n",
      "    Mappings loaded from file\n",
      "   Users: 137,883\n",
      "   Movies: 34,461\n",
      "\n",
      " Loading evaluation setup...\n",
      " Evaluation setup loaded:\n",
      "   Evaluation users: 100\n",
      "   Ground truth available: 100 users\n",
      "\n",
      " Previous CF Results (for comparison):\n",
      "   User-Based CF:\n",
      "      • Precision@10: 1.73%\n",
      "      • Hit Rate@10:  16.33%\n",
      "   Item-Based CF:\n",
      "      • Precision@10: 14.08%\n",
      "      • Hit Rate@10:  48.98%\n",
      "\n",
      " Loading movies data...\n",
      " Movies loaded: 62,423 movies\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# IMPORT LIBRARIES\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SETUP: IMPORTING LIBRARIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "import numpy as np              \n",
    "import pandas as pd            \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns          \n",
    "import pickle                \n",
    "import os                     \n",
    "import time                    \n",
    "from datetime import datetime  \n",
    "\n",
    "# Sparse matrix operations\n",
    "from scipy.sparse import csr_matrix, load_npz  \n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.decomposition import TruncatedSVD  \n",
    "from sklearn.preprocessing import normalize     \n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\" All libraries imported successfully\")\n",
    "\n",
    " \n",
    "# VISUALIZATION SETTINGS\n",
    "\n",
    "\n",
    "# Set consistent, professional plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Default figure settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['savefig.bbox'] = 'tight'\n",
    "\n",
    "print(\" Plot settings configured\")\n",
    "\n",
    "\n",
    "# DIRECTORY STRUCTURE \n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIRECTORY SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "\n",
    "import sys\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())  \n",
    "\n",
    "print(f\" Current directory: {os.getcwd()}\")\n",
    "print(f\" Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Define project directories relative to PROJECT_ROOT\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, 'data')\n",
    "PROCESSED_DIR = os.path.join(PROJECT_ROOT, 'data', 'processed')\n",
    "MODELS_DIR = os.path.join(PROJECT_ROOT, 'models')\n",
    "RESULTS_DIR = os.path.join(PROJECT_ROOT, 'reports', 'results')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [MODELS_DIR, RESULTS_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "print(f\"\\n Directory structure:\")\n",
    "print(f\"   • Data: {DATA_DIR}\")\n",
    "print(f\"   • Processed: {PROCESSED_DIR}\")\n",
    "print(f\"   • Models: {MODELS_DIR}\")\n",
    "print(f\"   • Results: {RESULTS_DIR}\")\n",
    "\n",
    "# Verify directories exist\n",
    "all_exist = all(os.path.exists(d) for d in [DATA_DIR, MODELS_DIR])\n",
    "if all_exist:\n",
    "    print(f\" All directories ready\")\n",
    "else:\n",
    "    print(f\"  Some directories missing - will try to continue\")\n",
    "\n",
    "# LOAD PREVIOUS RESULTS\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LOADING DATA FROM PREVIOUS NOTEBOOKS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load User-Item Matrix\n",
    "\n",
    "print(\"\\n Loading user-item interaction matrix...\")\n",
    "\n",
    "matrix_path = os.path.join(MODELS_DIR, 'user_item_matrix.npz')\n",
    "\n",
    "print(f\"   Looking for: {matrix_path}\")\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(matrix_path):\n",
    "    print(f\"    Matrix not found!\")\n",
    "    print(f\"\\n  SOLUTION: We need to generate the matrix from Notebook 5 data\")\n",
    "    print(f\"   I'll help you create it now...\\n\")\n",
    "    \n",
    "    # Try to load from training data\n",
    "    train_path = os.path.join(DATA_DIR, 'ratings_train.csv')\n",
    "    \n",
    "    if os.path.exists(train_path):\n",
    "        print(\"   ✅ Found training data - will regenerate matrix\")\n",
    "        print(\"   This will take a few minutes...\")\n",
    "        \n",
    "        # Load training data\n",
    "        train = pd.read_csv(train_path)\n",
    "        print(f\"   Loaded: {len(train):,} training ratings\")\n",
    "        \n",
    "        # Create mappings\n",
    "        unique_users = train['userId'].unique()\n",
    "        unique_movies = train['movieId'].unique()\n",
    "        \n",
    "        user_to_idx = {user_id: idx for idx, user_id in enumerate(unique_users)}\n",
    "        idx_to_user = {idx: user_id for user_id, idx in user_to_idx.items()}\n",
    "        movie_to_idx = {movie_id: idx for idx, movie_id in enumerate(unique_movies)}\n",
    "        idx_to_movie = {idx: movie_id for movie_id, idx in movie_to_idx.items()}\n",
    "        \n",
    "        print(f\"   Created mappings: {len(user_to_idx):,} users, {len(movie_to_idx):,} movies\")\n",
    "        \n",
    "        # Build matrix\n",
    "        n_users = len(user_to_idx)\n",
    "        n_movies = len(movie_to_idx)\n",
    "        \n",
    "        user_indices = train['userId'].map(user_to_idx).values\n",
    "        movie_indices = train['movieId'].map(movie_to_idx).values\n",
    "        ratings = train['rating'].values\n",
    "        \n",
    "        user_item_matrix = csr_matrix(\n",
    "            (ratings, (user_indices, movie_indices)),\n",
    "            shape=(n_users, n_movies)\n",
    "        )\n",
    "        \n",
    "        print(f\"   Matrix created: {user_item_matrix.shape}\")\n",
    "        \n",
    "        # Save for next time\n",
    "        from scipy.sparse import save_npz\n",
    "        save_npz(matrix_path, user_item_matrix)\n",
    "        print(f\"    Saved matrix for future use\")\n",
    "        \n",
    "        # Save mappings\n",
    "        mappings = {\n",
    "            'user_to_idx': user_to_idx,\n",
    "            'idx_to_user': idx_to_user,\n",
    "            'movie_to_idx': movie_to_idx,\n",
    "            'idx_to_movie': idx_to_movie\n",
    "        }\n",
    "        mappings_path = os.path.join(MODELS_DIR, 'matrix_mappings.pkl')\n",
    "        with open(mappings_path, 'wb') as f:\n",
    "            pickle.dump(mappings, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"    Saved mappings for future use\")\n",
    "        \n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cannot find training data at {train_path}. \"\n",
    "            \"Please ensure Notebook 5 has been run and data files exist.\"\n",
    "        )\n",
    "else:\n",
    "    # Load existing matrix\n",
    "    user_item_matrix = load_npz(matrix_path)\n",
    "    print(f\"    Matrix loaded from file\")\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = (1 - user_item_matrix.nnz / \n",
    "            (user_item_matrix.shape[0] * user_item_matrix.shape[1])) * 100\n",
    "\n",
    "print(f\"\\n Matrix ready:\")\n",
    "print(f\"   Shape: {user_item_matrix.shape[0]:,} users × \"\n",
    "      f\"{user_item_matrix.shape[1]:,} movies\")\n",
    "print(f\"   Non-zero entries: {user_item_matrix.nnz:,}\")\n",
    "print(f\"   Sparsity: {sparsity:.2f}%\")\n",
    "print(f\"   Memory: {user_item_matrix.data.nbytes / (1024**2):.1f} MB\")\n",
    "\n",
    "#\n",
    "# Load ID Mappings\n",
    "\n",
    "print(\"\\n  Loading user/movie ID mappings...\")\n",
    "\n",
    "mappings_path = os.path.join(MODELS_DIR, 'matrix_mappings.pkl')\n",
    "\n",
    "if not os.path.exists(mappings_path):\n",
    "    print(f\"     Mappings not found - using ones created above\")\n",
    "    # Already created above when regenerating matrix\n",
    "else:\n",
    "    with open(mappings_path, 'rb') as f:\n",
    "        mappings = pickle.load(f)\n",
    "        user_to_idx = mappings['user_to_idx']\n",
    "        idx_to_user = mappings['idx_to_user']\n",
    "        movie_to_idx = mappings['movie_to_idx']\n",
    "        idx_to_movie = mappings['idx_to_movie']\n",
    "    print(f\"    Mappings loaded from file\")\n",
    "\n",
    "print(f\"   Users: {len(user_to_idx):,}\")\n",
    "print(f\"   Movies: {len(movie_to_idx):,}\")\n",
    "\n",
    "# Load Evaluation Setup \n",
    "\n",
    "print(\"\\n Loading evaluation setup...\")\n",
    "\n",
    "cf_results_path = os.path.join(MODELS_DIR, 'evaluation_results_cf.pkl')\n",
    "\n",
    "if os.path.exists(cf_results_path):\n",
    "    with open(cf_results_path, 'rb') as f:\n",
    "        cf_results = pickle.load(f)\n",
    "        eval_user_ids = cf_results['eval_user_ids']\n",
    "        ground_truth = cf_results['ground_truth']\n",
    "        ub_metrics = cf_results['user_based_metrics']\n",
    "        ib_metrics = cf_results['item_based_metrics']\n",
    "    \n",
    "    print(f\" Evaluation setup loaded:\")\n",
    "    print(f\"   Evaluation users: {len(eval_user_ids)}\")\n",
    "    print(f\"   Ground truth available: {len(ground_truth)} users\")\n",
    "    \n",
    "    print(f\"\\n Previous CF Results (for comparison):\")\n",
    "    print(f\"   User-Based CF:\")\n",
    "    print(f\"      • Precision@10: {ub_metrics['Precision@10']:.2f}%\")\n",
    "    print(f\"      • Hit Rate@10:  {ub_metrics['Hit Rate@10']:.2f}%\")\n",
    "    print(f\"   Item-Based CF:\")\n",
    "    print(f\"      • Precision@10: {ib_metrics['Precision@10']:.2f}%\")\n",
    "    print(f\"      • Hit Rate@10:  {ib_metrics['Hit Rate@10']:.2f}%\")\n",
    "else:\n",
    "    print(f\"     CF results not found\")\n",
    "    print(f\"   We'll create evaluation setup from test data...\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_path = os.path.join(DATA_DIR, 'ratings_test.csv')\n",
    "    if os.path.exists(test_path):\n",
    "        test = pd.read_csv(test_path)\n",
    "        \n",
    "        # Sample 100 users who exist in training\n",
    "        test_users = test[test['userId'].isin(user_to_idx.keys())]['userId'].unique()\n",
    "        np.random.seed(42)\n",
    "        eval_user_ids = np.random.choice(test_users, size=min(100, len(test_users)), replace=False)\n",
    "        \n",
    "        # Create ground truth\n",
    "        ground_truth = {}\n",
    "        for user_id in eval_user_ids:\n",
    "            user_test = test[test['userId'] == user_id]\n",
    "            relevant = user_test[user_test['rating'] >= 4.0]['movieId'].tolist()\n",
    "            ground_truth[user_id] = relevant\n",
    "        \n",
    "        print(f\"    Created evaluation setup: {len(eval_user_ids)} users\")\n",
    "        \n",
    "        # No previous CF metrics\n",
    "        ub_metrics = None\n",
    "        ib_metrics = None\n",
    "    else:\n",
    "        print(f\"    Cannot create evaluation - test data not found\")\n",
    "        eval_user_ids = []\n",
    "        ground_truth = {}\n",
    "        ub_metrics = None\n",
    "        ib_metrics = None\n",
    "\n",
    "\n",
    "# Load Movies Data\n",
    "\n",
    "print(\"\\n Loading movies data...\")\n",
    "\n",
    "movies_path = os.path.join(PROCESSED_DIR, 'movies_features.csv')\n",
    "if not os.path.exists(movies_path):\n",
    "    movies_path = os.path.join(DATA_DIR, 'movies_filtered.csv')\n",
    "if not os.path.exists(movies_path):\n",
    "    movies_path = os.path.join(DATA_DIR, 'movies.csv')\n",
    "\n",
    "if os.path.exists(movies_path):\n",
    "    movies = pd.read_csv(movies_path)\n",
    "    print(f\" Movies loaded: {len(movies):,} movies\")\n",
    "else:\n",
    "    print(f\"  Movies file not found - will work without titles\")\n",
    "    movies = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b673f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
